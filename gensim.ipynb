{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Saudis',\n",
       " 'are',\n",
       " 'preparing',\n",
       " 'a',\n",
       " 'report',\n",
       " 'that',\n",
       " 'will',\n",
       " 'acknowledge',\n",
       " 'that']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 400000,\n",
       " 'file_size': 69182535,\n",
       " 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'parameters': {'dimension': 50},\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       " 'file_name': 'glove-wiki-gigaword-50.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
